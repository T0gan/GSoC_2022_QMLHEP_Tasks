{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAqtfBqH4xlP",
        "outputId": "b0bd622d-cb74-4446-8597-a3af057e234c"
      },
      "id": "qAqtfBqH4xlP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -q tensorflow-quantum\n",
        "!pip uninstall -q tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZWW0tBaVGeq",
        "outputId": "1007032b-7e96-4198-fd84-d270718be288"
      },
      "id": "rZWW0tBaVGeq",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed (y/n)? y\n",
            "Proceed (y/n)? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cirq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0CtZJRnxhSDe",
        "outputId": "03598375-445d-48c8-b5f8-b130f4762c6c"
      },
      "id": "0CtZJRnxhSDe",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cirq\n",
            "  Downloading cirq-0.14.1-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: cirq-google==0.14.1 in /usr/local/lib/python3.7/dist-packages (from cirq) (0.14.1)\n",
            "Requirement already satisfied: cirq-core==0.14.1 in /usr/local/lib/python3.7/dist-packages (from cirq) (0.14.1)\n",
            "Collecting cirq-aqt==0.14.1\n",
            "  Downloading cirq_aqt-0.14.1-py3-none-any.whl (19 kB)\n",
            "Collecting cirq-web==0.14.1\n",
            "  Downloading cirq_web-0.14.1-py3-none-any.whl (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting cirq-pasqal==0.14.1\n",
            "  Downloading cirq_pasqal-0.14.1-py3-none-any.whl (30 kB)\n",
            "Collecting cirq-ionq==0.14.1\n",
            "  Downloading cirq_ionq-0.14.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting cirq-rigetti==0.14.1\n",
            "  Downloading cirq_rigetti-0.14.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests~=2.18 in /usr/local/lib/python3.7/dist-packages (from cirq-aqt==0.14.1->cirq) (2.23.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (3.2.2)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (1.4.1)\n",
            "Requirement already satisfied: backports.cached-property~=1.0.1 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (4.64.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (1.3.5)\n",
            "Requirement already satisfied: duet~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (0.2.5)\n",
            "Requirement already satisfied: sympy<1.10 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (1.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (3.10.0.0)\n",
            "Requirement already satisfied: networkx~=2.4 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (2.6.3)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.7/dist-packages (from cirq-core==0.14.1->cirq) (2.4.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from cirq-google==0.14.1->cirq) (1.21.0)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from cirq-google==0.14.1->cirq) (3.17.3)\n",
            "Collecting httpcore~=0.11.1\n",
            "  Downloading httpcore-0.11.1-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting six~=1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting h11~=0.9.0\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting pyjwt~=1.7.1\n",
            "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting retrying~=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Collecting sniffio~=1.2.0\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna~=2.10 in /usr/local/lib/python3.7/dist-packages (from cirq-rigetti==0.14.1->cirq) (2.10)\n",
            "Collecting qcs-api-client~=0.8.0\n",
            "  Downloading qcs_api_client-0.8.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting iso8601~=0.1.14\n",
            "  Downloading iso8601-0.1.16-py2.py3-none-any.whl (10 kB)\n",
            "Collecting rfc3339~=6.2\n",
            "  Downloading rfc3339-6.2-py3-none-any.whl (5.5 kB)\n",
            "Collecting certifi~=2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting toml~=0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rfc3986~=1.5.0\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting attrs~=20.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting httpx~=0.15.5\n",
            "  Downloading httpx-0.15.5-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting pydantic~=1.8.2\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 49.2 MB/s \n",
            "\u001b[?25hCollecting pyquil~=3.0.0\n",
            "  Downloading pyquil-3.0.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil~=2.8.1 in /usr/local/lib/python3.7/dist-packages (from cirq-rigetti==0.14.1->cirq) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (2018.9)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (1.52.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (1.18.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (1.32.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (4.2.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.14.1->cirq) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.14.1->cirq) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq-core==0.14.1->cirq) (3.0.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq-google==0.14.1->cirq) (0.4.8)\n",
            "Collecting numpy~=1.16\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 51.7 MB/s \n",
            "\u001b[?25hCollecting retry<0.10.0,>=0.9.2\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.3\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting lark<0.12.0,>=0.11.1\n",
            "  Downloading lark-0.11.3.tar.gz (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting rpcq<4.0.0,>=3.6.0\n",
            "  Downloading rpcq-3.10.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 256 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.3->pyquil~=3.0.0->cirq-rigetti==0.14.1->cirq) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.18->cirq-aqt==0.14.1->cirq) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.18->cirq-aqt==0.14.1->cirq) (3.0.4)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from retry<0.10.0,>=0.9.2->pyquil~=3.0.0->cirq-rigetti==0.14.1->cirq) (4.4.2)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.7/dist-packages (from retry<0.10.0,>=0.9.2->pyquil~=3.0.0->cirq-rigetti==0.14.1->cirq) (1.11.0)\n",
            "Collecting msgpack<1.0,>=0.6\n",
            "  Downloading msgpack-0.6.2-cp37-cp37m-manylinux1_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting python-rapidjson\n",
            "  Downloading python_rapidjson-1.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from rpcq<4.0.0,>=3.6.0->pyquil~=3.0.0->cirq-rigetti==0.14.1->cirq) (22.3.0)\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy<1.10->cirq-core==0.14.1->cirq) (1.2.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 65.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: lark, retrying, rpcq\n",
            "  Building wheel for lark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lark: filename=lark-0.11.3-py2.py3-none-any.whl size=99648 sha256=92ae3fa24a2170937741e4a3bd6ccf88dd2ec44f16148411ad6af1e74462d5db\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/61/3c/9ac365f55966367be8d77dbeb21a3ddece3c466e660121e8d6\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=3d1227b97de9b36e3b072b3caf32f4a5048201fe0a0f31498616c257af073daa\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for rpcq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpcq: filename=rpcq-3.10.0-py3-none-any.whl size=45969 sha256=e73c6fdd2a70ce9fbea1160a7e14b7c369f2fd0595a6c18b82546a1675dba7d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/44/60/4b8ce5700839431ee56820af595a2ac0e013a180232e5afba6\n",
            "Successfully built lark retrying rpcq\n",
            "Installing collected packages: sniffio, six, rfc3986, h11, ruamel.yaml.clib, httpcore, certifi, toml, ruamel.yaml, rfc3339, retrying, python-rapidjson, pyjwt, pydantic, numpy, msgpack, iso8601, httpx, attrs, scipy, rpcq, retry, qcs-api-client, lark, importlib-metadata, pyquil, cirq-web, cirq-rigetti, cirq-pasqal, cirq-ionq, cirq-aqt, cirq\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.3\n",
            "    Uninstalling msgpack-1.0.3:\n",
            "      Successfully uninstalled msgpack-1.0.3\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.4.0\n",
            "    Uninstalling attrs-21.4.0:\n",
            "      Successfully uninstalled attrs-21.4.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, which is not installed.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed attrs-20.3.0 certifi-2021.5.30 cirq-0.14.1 cirq-aqt-0.14.1 cirq-ionq-0.14.1 cirq-pasqal-0.14.1 cirq-rigetti-0.14.1 cirq-web-0.14.1 h11-0.9.0 httpcore-0.11.1 httpx-0.15.5 importlib-metadata-3.10.1 iso8601-0.1.16 lark-0.11.3 msgpack-0.6.2 numpy-1.21.6 pydantic-1.8.2 pyjwt-1.7.1 pyquil-3.0.1 python-rapidjson-1.6 qcs-api-client-0.8.0 retry-0.9.2 retrying-1.3.3 rfc3339-6.2 rfc3986-1.5.0 rpcq-3.10.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 scipy-1.7.3 six-1.16.0 sniffio-1.2.0 toml-0.10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "numpy",
                  "scipy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow-quantum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naTwat-rhgfS",
        "outputId": "a8e2467a-294d-4e80-e360-2bfb7c80ba90"
      },
      "id": "naTwat-rhgfS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 497.5 MB 17 kB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 52.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyquil 3.0.1 requires importlib-metadata<4.0.0,>=3.7.3; python_version < \"3.8\", but you have importlib-metadata 4.11.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cirq\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "import sympy\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "from cirq.ops import *"
      ],
      "metadata": {
        "id": "Kpx221gSuZlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a57b18-4bc9-413f-91de-084a2eb3bac0"
      },
      "id": "Kpx221gSuZlp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: cirq.ops.moment was used but is deprecated.\n",
            " it will be removed in cirq v0.16.\n",
            " Use cirq.circuits.moment instead.\n",
            "\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task II: Quantum Generative Adversarial Network (QGAN) Part\n",
        "\n",
        "You will explore how best to apply a quantum generative adversarial network (QGAN) to solve a High Energy Data analysis issue, more specifically, separating the signal events from the background events. You should use the Google Cirq and Tensorflow Quantum (TFQ) libraries for this task.\n",
        "A set of input samples (simulated with Delphes) is provided in NumPy NPZ format. In the input file, there are only 100 samples for training and 100 samples for testing so it won’t take much computing resources to accomplish this\n",
        "task. The signal events are labeled with 1 while the background events are labeled with 0.\n",
        "Be sure to show that you understand how to fine tune your machine learning model to improve the performance. The performance can be evaluated with classification accuracy or Area Under ROC Curve (AUC)."
      ],
      "metadata": {
        "id": "OgsdjjAs4APX"
      },
      "id": "OgsdjjAs4APX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "with np.load('/content/drive/MyDrive/Colab Notebooks/ML4SCI Tasks/QIS_EXAM_200Events.npz', allow_pickle=True) as data:\n",
        "    x_train = data[\"training_input\"].item()\n",
        "    x_test = data[\"test_input\"].item()"
      ],
      "metadata": {
        "id": "mzN2bSd4vzhx"
      },
      "id": "mzN2bSd4vzhx",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_0 = x_train['0']\n",
        "x_train_1 = x_train['1']\n",
        "x_test_0 = x_test['0']\n",
        "x_test_1 = x_test['1']\n",
        "\n",
        "x_train = np.concatenate((x_train_0, x_train_1), axis=0)\n",
        "x_test = np.concatenate((x_test_0, x_test_1), axis=0)\n",
        "y_train = np.zeros((len(x_train),), dtype=np.int32)\n",
        "y_train[len(x_train_0):] = 1\n",
        "y_test = np.zeros((len(x_test),), dtype=np.int32)\n",
        "y_test[len(x_test_0):] = 1\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "print(x_train[:10])\n",
        "print(x_test[:10])"
      ],
      "metadata": {
        "id": "5-11V1DzuZvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5a117e-7673-4c3f-d57c-0564ec779d16"
      },
      "id": "5-11V1DzuZvi",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 5) (100, 5) (100,) (100,)\n",
            "[[-0.43079088  0.86834819 -0.92614721 -0.92662029 -0.56900862]\n",
            " [ 0.33924198  0.56155499  0.93097459 -0.91631726 -0.54463516]\n",
            " [-0.42888879  0.87064961 -0.92782179 -0.77533991 -0.58329176]\n",
            " [-0.43262871  0.86128919 -0.92240878 -0.88048862 -0.49963115]\n",
            " [-0.99925345 -0.99949586  0.07753685 -0.84218034 -0.5149399 ]\n",
            " [-0.99631106 -0.99775978  0.0756427  -0.54117216 -0.66299335]\n",
            " [-0.42645921  0.87141204 -0.92908723 -0.52650143 -0.62187526]\n",
            " [ 0.34317906  0.57125045  0.92638556 -0.85113425 -0.40170562]\n",
            " [-0.99904849 -0.99933931  0.07737929 -0.81161066 -0.53550246]\n",
            " [ 0.3371327   0.55874622  0.92996976 -0.9117092  -0.50996097]]\n",
            "[[-0.43080401  0.86308617 -0.92383665 -0.7288808  -0.53944676]\n",
            " [ 0.33955674  0.5637837   0.92720881 -0.81843942 -0.6058888 ]\n",
            " [ 0.88826449 -0.94407139 -0.66863101 -0.75178845 -0.60501115]\n",
            " [-0.99698887 -0.99830749  0.07614012 -0.64736972 -0.61107892]\n",
            " [-0.42987561  0.86729839 -0.92592964 -0.77358656 -0.45954928]\n",
            " [-0.99903924 -0.99940754  0.07740513 -0.81484426 -0.59764327]\n",
            " [ 0.89527424 -0.93164945 -0.67830638 -0.17547695  0.58070215]\n",
            " [-0.99867731 -0.99920143  0.07716389 -0.78608448 -0.58575145]\n",
            " [-0.99954016 -0.99970837  0.07769953 -0.86504334 -0.59019686]\n",
            " [-0.99933588 -0.99957473  0.07758163 -0.84371294 -0.58384261]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_circuit(qubits):\n",
        "  \"Generate a random circuit\"\n",
        "  random_circuit = cirq.generate_boixo_2018_supremacy_circuits_v2(qubits, cz_depth=2, seed=1234)\n",
        "  return random_circuit\n",
        "\n",
        "def generate_data(circuit, n_samples):\n",
        "  \"Given # of samples from circuit to a tensor\"\n",
        "  return tf.squeeze(tfq.layers.Sample()(circuit, repetitions=n_samples).to_tensor())"
      ],
      "metadata": {
        "id": "pvVFzyGJhLR6"
      },
      "id": "pvVFzyGJhLR6",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 5\n",
        "n_samples = 100\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "reference_circuit = generate_circuit(qubits)\n",
        "random_data = generate_data(reference_circuit, n_samples)\n",
        "print(random_data.shape)\n",
        "random_data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wfMLtO47DBC",
        "outputId": "39fcd6ee-743c-442d-fe0f-96c337c7f644"
      },
      "id": "8wfMLtO47DBC",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 5), dtype=int8, numpy=\n",
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1]], dtype=int8)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator():\n",
        "  \"Generator\"\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(256, use_bias=False, input_shape=(n_qubits,), activation='ReLU'))\n",
        "  model.add(tf.keras.layers.Dense(128, activation='ReLU'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.Dense(64, activation='ReLU'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.Dense(n_qubits, activation='tanh'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def make_discriminator():\n",
        "  \"Discriminator\"\n",
        "  input = tf.keras.Input(shape=(n_qubits, ), dtype=tf.float32)\n",
        "  output = tf.keras.layers.Dense(256, use_bias=False, activation='ReLU')(input)\n",
        "  output = tf.keras.layers.Dense(128, activation='ReLU')(output)\n",
        "  output = tf.keras.layers.Dropout(0.4)(output)\n",
        "  output = tf.keras.layers.Dense(64, activation='ReLU')(output)\n",
        "  output = tf.keras.layers.Dropout(0.3)(output)\n",
        "  classification = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
        "  discrimination = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n",
        "  model = tf.keras.Model(inputs=[input], outputs=[discrimination, classification])\n",
        "  return model"
      ],
      "metadata": {
        "id": "k-ThOcTh7DrQ"
      },
      "id": "k-ThOcTh7DrQ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5)"
      ],
      "metadata": {
        "id": "L695zXI17Du1"
      },
      "id": "L695zXI17Du1",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20\n",
        "cross_entropy_t = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels, noise):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
        "    generated_data = generator(noise, training=True)\n",
        "\n",
        "    real_output, real_preds = discriminator(images, training=True)\n",
        "    fake_output, fake_preds = discriminator(generated_data, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    dis_loss = discriminator_loss(real_output, fake_output)\n",
        "    dis_loss = dis_loss + cross_entropy_t(tf.one_hot(tf.squeeze(labels), depth=2), real_preds)\n",
        "  \n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "  return gen_loss, dis_loss"
      ],
      "metadata": {
        "id": "3NWmJKei7DDr"
      },
      "id": "3NWmJKei7DDr",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, labels, noise, epochs):\n",
        "  \"Training model\"\n",
        "  batched_data = tf.data.Dataset.from_tensor_slices(data).batch(BATCH_SIZE)\n",
        "  batched_labels = tf.data.Dataset.from_tensor_slices(labels).batch(BATCH_SIZE)\n",
        "  batched_noise = tf.data.Dataset.from_tensor_slices(noise).batch(BATCH_SIZE)\n",
        "  AUC = tf.keras.metrics.AUC()\n",
        "  g_losses = []\n",
        "  d_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    g_epoch_losses = []\n",
        "    d_epoch_losses = []\n",
        "    # aucs_epoch = []\n",
        "    for i, (data_batch, labels_batch, noise_batch) in enumerate(zip(batched_data, batched_labels, batched_noise)):\n",
        "      gl, dl = train_step(data_batch, labels_batch, noise_batch)\n",
        "      g_epoch_losses.append(gl)\n",
        "      d_epoch_losses.append(dl)\n",
        "    \n",
        "    g_losses.append(tf.reduce_mean(g_epoch_losses))\n",
        "    d_losses.append(tf.reduce_mean(d_epoch_losses))\n",
        "    print('For Epoch: {}, Generator loss: {} and Discriminator loss: {}'.format(epoch, tf.reduce_mean(g_epoch_losses), tf.reduce_mean(d_epoch_losses)))\n",
        "  return g_losses, d_losses"
      ],
      "metadata": {
        "id": "z84uXEXe7DGe"
      },
      "id": "z84uXEXe7DGe",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "gen_losse, dis_losse = train(x_train, y_train, random_data, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQIiYlfv7DJH",
        "outputId": "0bd2624a-b8ea-49c4-f0e9-0945c1037c74"
      },
      "id": "FQIiYlfv7DJH",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Epoch: 0, Generator loss: 4.809860706329346 and Discriminator loss: 0.8590350151062012\n",
            "For Epoch: 1, Generator loss: 4.9820098876953125 and Discriminator loss: 0.867115318775177\n",
            "For Epoch: 2, Generator loss: 4.841441631317139 and Discriminator loss: 0.7906109690666199\n",
            "For Epoch: 3, Generator loss: 5.351897239685059 and Discriminator loss: 0.8700622320175171\n",
            "For Epoch: 4, Generator loss: 5.909662246704102 and Discriminator loss: 0.7579900622367859\n",
            "For Epoch: 5, Generator loss: 5.450976371765137 and Discriminator loss: 0.8505834341049194\n",
            "For Epoch: 6, Generator loss: 5.556309700012207 and Discriminator loss: 0.8474451303482056\n",
            "For Epoch: 7, Generator loss: 4.947591781616211 and Discriminator loss: 0.8580986857414246\n",
            "For Epoch: 8, Generator loss: 5.128631591796875 and Discriminator loss: 0.9197665452957153\n",
            "For Epoch: 9, Generator loss: 5.140061378479004 and Discriminator loss: 0.8726819753646851\n",
            "For Epoch: 10, Generator loss: 5.211541652679443 and Discriminator loss: 0.8258367776870728\n",
            "For Epoch: 11, Generator loss: 5.123106002807617 and Discriminator loss: 0.8173900842666626\n",
            "For Epoch: 12, Generator loss: 5.607227325439453 and Discriminator loss: 0.7816060781478882\n",
            "For Epoch: 13, Generator loss: 4.637063026428223 and Discriminator loss: 0.9497394561767578\n",
            "For Epoch: 14, Generator loss: 5.280007839202881 and Discriminator loss: 0.9077544212341309\n",
            "For Epoch: 15, Generator loss: 5.6833953857421875 and Discriminator loss: 0.9035907983779907\n",
            "For Epoch: 16, Generator loss: 4.88065242767334 and Discriminator loss: 0.9686828851699829\n",
            "For Epoch: 17, Generator loss: 5.440075874328613 and Discriminator loss: 0.8380312919616699\n",
            "For Epoch: 18, Generator loss: 4.964402675628662 and Discriminator loss: 0.8950567245483398\n",
            "For Epoch: 19, Generator loss: 5.115321159362793 and Discriminator loss: 0.8645407557487488\n",
            "For Epoch: 20, Generator loss: 5.342788219451904 and Discriminator loss: 0.7777196764945984\n",
            "For Epoch: 21, Generator loss: 5.327808380126953 and Discriminator loss: 0.7977818250656128\n",
            "For Epoch: 22, Generator loss: 4.950601577758789 and Discriminator loss: 0.8508075475692749\n",
            "For Epoch: 23, Generator loss: 5.552999019622803 and Discriminator loss: 0.8256083726882935\n",
            "For Epoch: 24, Generator loss: 5.389208793640137 and Discriminator loss: 0.8225700259208679\n",
            "For Epoch: 25, Generator loss: 5.0784807205200195 and Discriminator loss: 0.9016722440719604\n",
            "For Epoch: 26, Generator loss: 5.539076805114746 and Discriminator loss: 0.8329240679740906\n",
            "For Epoch: 27, Generator loss: 5.161064624786377 and Discriminator loss: 0.8683716058731079\n",
            "For Epoch: 28, Generator loss: 4.338332176208496 and Discriminator loss: 0.916728675365448\n",
            "For Epoch: 29, Generator loss: 5.676352024078369 and Discriminator loss: 0.9026175737380981\n",
            "For Epoch: 30, Generator loss: 4.758808135986328 and Discriminator loss: 0.8449960947036743\n",
            "For Epoch: 31, Generator loss: 5.074774742126465 and Discriminator loss: 0.8667764663696289\n",
            "For Epoch: 32, Generator loss: 5.561730861663818 and Discriminator loss: 0.8530085682868958\n",
            "For Epoch: 33, Generator loss: 5.416337013244629 and Discriminator loss: 0.8374361991882324\n",
            "For Epoch: 34, Generator loss: 4.957193851470947 and Discriminator loss: 0.8270066976547241\n",
            "For Epoch: 35, Generator loss: 5.057024955749512 and Discriminator loss: 0.9134092330932617\n",
            "For Epoch: 36, Generator loss: 5.009894371032715 and Discriminator loss: 0.8106170892715454\n",
            "For Epoch: 37, Generator loss: 4.859788417816162 and Discriminator loss: 0.8479042053222656\n",
            "For Epoch: 38, Generator loss: 5.269642353057861 and Discriminator loss: 0.8427923321723938\n",
            "For Epoch: 39, Generator loss: 5.252490520477295 and Discriminator loss: 0.8028272390365601\n",
            "For Epoch: 40, Generator loss: 4.827676773071289 and Discriminator loss: 0.9448636174201965\n",
            "For Epoch: 41, Generator loss: 4.9231109619140625 and Discriminator loss: 0.8083149194717407\n",
            "For Epoch: 42, Generator loss: 4.995520114898682 and Discriminator loss: 0.7937898635864258\n",
            "For Epoch: 43, Generator loss: 5.0706868171691895 and Discriminator loss: 0.7859748601913452\n",
            "For Epoch: 44, Generator loss: 5.06624174118042 and Discriminator loss: 0.8863474130630493\n",
            "For Epoch: 45, Generator loss: 5.624365329742432 and Discriminator loss: 0.7362169027328491\n",
            "For Epoch: 46, Generator loss: 5.330199241638184 and Discriminator loss: 0.8466687202453613\n",
            "For Epoch: 47, Generator loss: 5.468342304229736 and Discriminator loss: 0.8135201334953308\n",
            "For Epoch: 48, Generator loss: 5.315316200256348 and Discriminator loss: 0.8395971059799194\n",
            "For Epoch: 49, Generator loss: 4.919726371765137 and Discriminator loss: 0.8429059982299805\n",
            "For Epoch: 50, Generator loss: 5.132009983062744 and Discriminator loss: 0.8902102708816528\n",
            "For Epoch: 51, Generator loss: 6.04141902923584 and Discriminator loss: 0.8096345663070679\n",
            "For Epoch: 52, Generator loss: 5.851263999938965 and Discriminator loss: 0.8688488006591797\n",
            "For Epoch: 53, Generator loss: 5.4723100662231445 and Discriminator loss: 0.8826907873153687\n",
            "For Epoch: 54, Generator loss: 5.7659783363342285 and Discriminator loss: 0.815824031829834\n",
            "For Epoch: 55, Generator loss: 5.298121452331543 and Discriminator loss: 0.8490778803825378\n",
            "For Epoch: 56, Generator loss: 4.9250383377075195 and Discriminator loss: 0.8980165719985962\n",
            "For Epoch: 57, Generator loss: 5.310606956481934 and Discriminator loss: 0.8511806726455688\n",
            "For Epoch: 58, Generator loss: 5.2897186279296875 and Discriminator loss: 0.7925615906715393\n",
            "For Epoch: 59, Generator loss: 5.118433952331543 and Discriminator loss: 0.8641327619552612\n",
            "For Epoch: 60, Generator loss: 5.746556758880615 and Discriminator loss: 0.8626152276992798\n",
            "For Epoch: 61, Generator loss: 5.391563892364502 and Discriminator loss: 0.8559393882751465\n",
            "For Epoch: 62, Generator loss: 5.242955684661865 and Discriminator loss: 0.9052698016166687\n",
            "For Epoch: 63, Generator loss: 5.555079936981201 and Discriminator loss: 0.8937277793884277\n",
            "For Epoch: 64, Generator loss: 5.206953525543213 and Discriminator loss: 0.8578052520751953\n",
            "For Epoch: 65, Generator loss: 5.489802360534668 and Discriminator loss: 0.9257650375366211\n",
            "For Epoch: 66, Generator loss: 5.530734539031982 and Discriminator loss: 0.8474622964859009\n",
            "For Epoch: 67, Generator loss: 5.531822204589844 and Discriminator loss: 0.9162231683731079\n",
            "For Epoch: 68, Generator loss: 5.403153419494629 and Discriminator loss: 0.8289390802383423\n",
            "For Epoch: 69, Generator loss: 5.3336968421936035 and Discriminator loss: 0.8960374593734741\n",
            "For Epoch: 70, Generator loss: 5.470574855804443 and Discriminator loss: 0.8240350484848022\n",
            "For Epoch: 71, Generator loss: 5.001401424407959 and Discriminator loss: 0.8615297079086304\n",
            "For Epoch: 72, Generator loss: 4.904906272888184 and Discriminator loss: 0.8240368962287903\n",
            "For Epoch: 73, Generator loss: 5.609912872314453 and Discriminator loss: 0.8099511861801147\n",
            "For Epoch: 74, Generator loss: 5.427765846252441 and Discriminator loss: 0.8398372530937195\n",
            "For Epoch: 75, Generator loss: 5.923962593078613 and Discriminator loss: 0.7905271649360657\n",
            "For Epoch: 76, Generator loss: 5.543997764587402 and Discriminator loss: 0.8488887548446655\n",
            "For Epoch: 77, Generator loss: 5.573395729064941 and Discriminator loss: 0.8165581822395325\n",
            "For Epoch: 78, Generator loss: 5.4312543869018555 and Discriminator loss: 0.9326878786087036\n",
            "For Epoch: 79, Generator loss: 5.286866664886475 and Discriminator loss: 0.8200544118881226\n",
            "For Epoch: 80, Generator loss: 5.18375301361084 and Discriminator loss: 0.834303081035614\n",
            "For Epoch: 81, Generator loss: 5.373087406158447 and Discriminator loss: 0.7658020257949829\n",
            "For Epoch: 82, Generator loss: 4.898665428161621 and Discriminator loss: 0.8178898096084595\n",
            "For Epoch: 83, Generator loss: 5.016429901123047 and Discriminator loss: 0.8676201701164246\n",
            "For Epoch: 84, Generator loss: 5.145954608917236 and Discriminator loss: 0.8994175791740417\n",
            "For Epoch: 85, Generator loss: 5.893734931945801 and Discriminator loss: 0.8216158151626587\n",
            "For Epoch: 86, Generator loss: 5.784402847290039 and Discriminator loss: 0.7914898991584778\n",
            "For Epoch: 87, Generator loss: 5.382098197937012 and Discriminator loss: 0.8207318186759949\n",
            "For Epoch: 88, Generator loss: 5.641304969787598 and Discriminator loss: 0.8620277643203735\n",
            "For Epoch: 89, Generator loss: 5.212619781494141 and Discriminator loss: 0.8764068484306335\n",
            "For Epoch: 90, Generator loss: 5.173984050750732 and Discriminator loss: 0.8224687576293945\n",
            "For Epoch: 91, Generator loss: 5.016138076782227 and Discriminator loss: 0.9202650785446167\n",
            "For Epoch: 92, Generator loss: 4.930808067321777 and Discriminator loss: 0.8900911211967468\n",
            "For Epoch: 93, Generator loss: 5.697564125061035 and Discriminator loss: 0.880268394947052\n",
            "For Epoch: 94, Generator loss: 5.468896865844727 and Discriminator loss: 0.84123295545578\n",
            "For Epoch: 95, Generator loss: 5.6785149574279785 and Discriminator loss: 0.8792262077331543\n",
            "For Epoch: 96, Generator loss: 5.857457160949707 and Discriminator loss: 0.8392831087112427\n",
            "For Epoch: 97, Generator loss: 5.286237716674805 and Discriminator loss: 0.7554876208305359\n",
            "For Epoch: 98, Generator loss: 5.756901264190674 and Discriminator loss: 0.8136368989944458\n",
            "For Epoch: 99, Generator loss: 4.97293758392334 and Discriminator loss: 0.8088777661323547\n",
            "For Epoch: 100, Generator loss: 5.507723331451416 and Discriminator loss: 0.8086754083633423\n",
            "For Epoch: 101, Generator loss: 5.723825454711914 and Discriminator loss: 0.7512011528015137\n",
            "For Epoch: 102, Generator loss: 5.482453346252441 and Discriminator loss: 0.8877143859863281\n",
            "For Epoch: 103, Generator loss: 5.898448467254639 and Discriminator loss: 0.8108140230178833\n",
            "For Epoch: 104, Generator loss: 6.124442100524902 and Discriminator loss: 0.728326141834259\n",
            "For Epoch: 105, Generator loss: 5.533668041229248 and Discriminator loss: 0.7713812589645386\n",
            "For Epoch: 106, Generator loss: 6.153815269470215 and Discriminator loss: 0.7528894543647766\n",
            "For Epoch: 107, Generator loss: 5.429623603820801 and Discriminator loss: 0.8890354037284851\n",
            "For Epoch: 108, Generator loss: 5.4239501953125 and Discriminator loss: 0.7933343648910522\n",
            "For Epoch: 109, Generator loss: 5.099225044250488 and Discriminator loss: 0.8245970010757446\n",
            "For Epoch: 110, Generator loss: 5.200299263000488 and Discriminator loss: 0.8348428010940552\n",
            "For Epoch: 111, Generator loss: 5.039517879486084 and Discriminator loss: 0.7838767766952515\n",
            "For Epoch: 112, Generator loss: 5.401148319244385 and Discriminator loss: 0.861496090888977\n",
            "For Epoch: 113, Generator loss: 5.141026020050049 and Discriminator loss: 0.9335163831710815\n",
            "For Epoch: 114, Generator loss: 5.806353569030762 and Discriminator loss: 0.8194095492362976\n",
            "For Epoch: 115, Generator loss: 5.947139263153076 and Discriminator loss: 0.7468971610069275\n",
            "For Epoch: 116, Generator loss: 5.519092082977295 and Discriminator loss: 0.8242219686508179\n",
            "For Epoch: 117, Generator loss: 5.3566765785217285 and Discriminator loss: 0.8206861615180969\n",
            "For Epoch: 118, Generator loss: 5.741435527801514 and Discriminator loss: 0.8709996938705444\n",
            "For Epoch: 119, Generator loss: 5.610499382019043 and Discriminator loss: 0.8637434244155884\n",
            "For Epoch: 120, Generator loss: 6.0679030418396 and Discriminator loss: 0.753132164478302\n",
            "For Epoch: 121, Generator loss: 4.98529052734375 and Discriminator loss: 0.9392240643501282\n",
            "For Epoch: 122, Generator loss: 5.614182949066162 and Discriminator loss: 0.9257222414016724\n",
            "For Epoch: 123, Generator loss: 6.0579705238342285 and Discriminator loss: 0.79131680727005\n",
            "For Epoch: 124, Generator loss: 5.474390506744385 and Discriminator loss: 0.8471925854682922\n",
            "For Epoch: 125, Generator loss: 5.293373107910156 and Discriminator loss: 0.822791576385498\n",
            "For Epoch: 126, Generator loss: 5.383440971374512 and Discriminator loss: 0.9690580368041992\n",
            "For Epoch: 127, Generator loss: 5.382218360900879 and Discriminator loss: 0.824286937713623\n",
            "For Epoch: 128, Generator loss: 5.161365032196045 and Discriminator loss: 0.9414733052253723\n",
            "For Epoch: 129, Generator loss: 5.710312843322754 and Discriminator loss: 0.8425596952438354\n",
            "For Epoch: 130, Generator loss: 5.557751655578613 and Discriminator loss: 0.8462015986442566\n",
            "For Epoch: 131, Generator loss: 5.2557854652404785 and Discriminator loss: 0.8493263125419617\n",
            "For Epoch: 132, Generator loss: 5.309717655181885 and Discriminator loss: 0.8402630090713501\n",
            "For Epoch: 133, Generator loss: 5.577552795410156 and Discriminator loss: 0.8009336590766907\n",
            "For Epoch: 134, Generator loss: 5.746633052825928 and Discriminator loss: 0.8200047612190247\n",
            "For Epoch: 135, Generator loss: 5.1598052978515625 and Discriminator loss: 0.8413757085800171\n",
            "For Epoch: 136, Generator loss: 5.937644004821777 and Discriminator loss: 0.8022066354751587\n",
            "For Epoch: 137, Generator loss: 5.627016067504883 and Discriminator loss: 0.8324033617973328\n",
            "For Epoch: 138, Generator loss: 5.378780364990234 and Discriminator loss: 0.7837628126144409\n",
            "For Epoch: 139, Generator loss: 5.830412864685059 and Discriminator loss: 0.8258472681045532\n",
            "For Epoch: 140, Generator loss: 5.656927585601807 and Discriminator loss: 0.7591928243637085\n",
            "For Epoch: 141, Generator loss: 5.845792770385742 and Discriminator loss: 0.8565850257873535\n",
            "For Epoch: 142, Generator loss: 5.936897277832031 and Discriminator loss: 0.7956384420394897\n",
            "For Epoch: 143, Generator loss: 5.205233573913574 and Discriminator loss: 0.8665609359741211\n",
            "For Epoch: 144, Generator loss: 6.061954021453857 and Discriminator loss: 0.7680038213729858\n",
            "For Epoch: 145, Generator loss: 5.099000453948975 and Discriminator loss: 0.8445073366165161\n",
            "For Epoch: 146, Generator loss: 5.934441566467285 and Discriminator loss: 0.7744942307472229\n",
            "For Epoch: 147, Generator loss: 5.780857086181641 and Discriminator loss: 0.7791612148284912\n",
            "For Epoch: 148, Generator loss: 5.393239498138428 and Discriminator loss: 0.8207308053970337\n",
            "For Epoch: 149, Generator loss: 5.437847137451172 and Discriminator loss: 0.8874942660331726\n",
            "For Epoch: 150, Generator loss: 5.274292945861816 and Discriminator loss: 0.8520194888114929\n",
            "For Epoch: 151, Generator loss: 5.762877941131592 and Discriminator loss: 0.7636295557022095\n",
            "For Epoch: 152, Generator loss: 5.4341535568237305 and Discriminator loss: 0.6785559058189392\n",
            "For Epoch: 153, Generator loss: 5.520684719085693 and Discriminator loss: 0.7730766534805298\n",
            "For Epoch: 154, Generator loss: 5.3647027015686035 and Discriminator loss: 0.8367552757263184\n",
            "For Epoch: 155, Generator loss: 5.533487796783447 and Discriminator loss: 0.7504211664199829\n",
            "For Epoch: 156, Generator loss: 5.944602012634277 and Discriminator loss: 0.8584388494491577\n",
            "For Epoch: 157, Generator loss: 5.792262077331543 and Discriminator loss: 0.7788518071174622\n",
            "For Epoch: 158, Generator loss: 5.463405132293701 and Discriminator loss: 0.7871605753898621\n",
            "For Epoch: 159, Generator loss: 5.435184955596924 and Discriminator loss: 0.8180757761001587\n",
            "For Epoch: 160, Generator loss: 5.198602199554443 and Discriminator loss: 0.876552402973175\n",
            "For Epoch: 161, Generator loss: 6.014997959136963 and Discriminator loss: 0.7925547361373901\n",
            "For Epoch: 162, Generator loss: 6.123073577880859 and Discriminator loss: 0.8673752546310425\n",
            "For Epoch: 163, Generator loss: 5.5706071853637695 and Discriminator loss: 0.8104565739631653\n",
            "For Epoch: 164, Generator loss: 5.471001148223877 and Discriminator loss: 0.8186546564102173\n",
            "For Epoch: 165, Generator loss: 5.862622261047363 and Discriminator loss: 0.8048381805419922\n",
            "For Epoch: 166, Generator loss: 5.731428146362305 and Discriminator loss: 0.8535517454147339\n",
            "For Epoch: 167, Generator loss: 6.307488441467285 and Discriminator loss: 0.8630484342575073\n",
            "For Epoch: 168, Generator loss: 5.854840278625488 and Discriminator loss: 0.8196778297424316\n",
            "For Epoch: 169, Generator loss: 5.538346290588379 and Discriminator loss: 0.7762209177017212\n",
            "For Epoch: 170, Generator loss: 5.683315753936768 and Discriminator loss: 0.7626022100448608\n",
            "For Epoch: 171, Generator loss: 5.5768232345581055 and Discriminator loss: 0.867632269859314\n",
            "For Epoch: 172, Generator loss: 4.7903289794921875 and Discriminator loss: 0.8343504071235657\n",
            "For Epoch: 173, Generator loss: 6.2129807472229 and Discriminator loss: 0.7749399542808533\n",
            "For Epoch: 174, Generator loss: 5.038646697998047 and Discriminator loss: 0.8030536770820618\n",
            "For Epoch: 175, Generator loss: 5.279482364654541 and Discriminator loss: 0.8207074999809265\n",
            "For Epoch: 176, Generator loss: 5.209842205047607 and Discriminator loss: 0.9625543355941772\n",
            "For Epoch: 177, Generator loss: 6.1240949630737305 and Discriminator loss: 0.8066908121109009\n",
            "For Epoch: 178, Generator loss: 5.791380882263184 and Discriminator loss: 0.8266660571098328\n",
            "For Epoch: 179, Generator loss: 6.038066864013672 and Discriminator loss: 0.7376292943954468\n",
            "For Epoch: 180, Generator loss: 5.680712699890137 and Discriminator loss: 0.860751748085022\n",
            "For Epoch: 181, Generator loss: 5.59476375579834 and Discriminator loss: 0.8281545639038086\n",
            "For Epoch: 182, Generator loss: 5.532105445861816 and Discriminator loss: 0.7814422249794006\n",
            "For Epoch: 183, Generator loss: 5.210775375366211 and Discriminator loss: 0.871412456035614\n",
            "For Epoch: 184, Generator loss: 5.37957763671875 and Discriminator loss: 0.7639360427856445\n",
            "For Epoch: 185, Generator loss: 5.722777366638184 and Discriminator loss: 0.7539178133010864\n",
            "For Epoch: 186, Generator loss: 5.056962490081787 and Discriminator loss: 0.8365486264228821\n",
            "For Epoch: 187, Generator loss: 5.566173553466797 and Discriminator loss: 0.797458827495575\n",
            "For Epoch: 188, Generator loss: 5.109801292419434 and Discriminator loss: 0.8330326080322266\n",
            "For Epoch: 189, Generator loss: 6.297551155090332 and Discriminator loss: 0.8166267275810242\n",
            "For Epoch: 190, Generator loss: 5.840951919555664 and Discriminator loss: 0.8394438028335571\n",
            "For Epoch: 191, Generator loss: 5.931352138519287 and Discriminator loss: 0.8154781460762024\n",
            "For Epoch: 192, Generator loss: 6.402090549468994 and Discriminator loss: 0.8486613035202026\n",
            "For Epoch: 193, Generator loss: 5.865331172943115 and Discriminator loss: 0.7537239193916321\n",
            "For Epoch: 194, Generator loss: 5.713761806488037 and Discriminator loss: 0.8688250780105591\n",
            "For Epoch: 195, Generator loss: 5.421609401702881 and Discriminator loss: 0.8461718559265137\n",
            "For Epoch: 196, Generator loss: 6.681003570556641 and Discriminator loss: 0.7639464139938354\n",
            "For Epoch: 197, Generator loss: 5.181353569030762 and Discriminator loss: 0.866926372051239\n",
            "For Epoch: 198, Generator loss: 5.225502967834473 and Discriminator loss: 0.7233791351318359\n",
            "For Epoch: 199, Generator loss: 6.495226860046387 and Discriminator loss: 0.7359153032302856\n",
            "For Epoch: 200, Generator loss: 5.639412879943848 and Discriminator loss: 0.8297405242919922\n",
            "For Epoch: 201, Generator loss: 5.5575432777404785 and Discriminator loss: 0.7847767472267151\n",
            "For Epoch: 202, Generator loss: 6.703984260559082 and Discriminator loss: 0.7582646012306213\n",
            "For Epoch: 203, Generator loss: 6.286606788635254 and Discriminator loss: 0.8176099061965942\n",
            "For Epoch: 204, Generator loss: 6.1088547706604 and Discriminator loss: 0.8715924024581909\n",
            "For Epoch: 205, Generator loss: 5.812754154205322 and Discriminator loss: 0.7822272777557373\n",
            "For Epoch: 206, Generator loss: 5.667428016662598 and Discriminator loss: 0.8378516435623169\n",
            "For Epoch: 207, Generator loss: 5.910053730010986 and Discriminator loss: 0.8446599841117859\n",
            "For Epoch: 208, Generator loss: 5.813077449798584 and Discriminator loss: 0.8358456492424011\n",
            "For Epoch: 209, Generator loss: 5.8436174392700195 and Discriminator loss: 0.8104665875434875\n",
            "For Epoch: 210, Generator loss: 5.219245910644531 and Discriminator loss: 0.8337532877922058\n",
            "For Epoch: 211, Generator loss: 6.137523651123047 and Discriminator loss: 0.7943834066390991\n",
            "For Epoch: 212, Generator loss: 5.965853691101074 and Discriminator loss: 0.7696094512939453\n",
            "For Epoch: 213, Generator loss: 5.948969841003418 and Discriminator loss: 0.8038312196731567\n",
            "For Epoch: 214, Generator loss: 5.658690452575684 and Discriminator loss: 0.8631535768508911\n",
            "For Epoch: 215, Generator loss: 5.655440330505371 and Discriminator loss: 0.8312956690788269\n",
            "For Epoch: 216, Generator loss: 5.602681636810303 and Discriminator loss: 0.8977792859077454\n",
            "For Epoch: 217, Generator loss: 6.385714054107666 and Discriminator loss: 0.7314425706863403\n",
            "For Epoch: 218, Generator loss: 5.558446407318115 and Discriminator loss: 0.824436366558075\n",
            "For Epoch: 219, Generator loss: 6.141502380371094 and Discriminator loss: 0.7667185664176941\n",
            "For Epoch: 220, Generator loss: 5.70743465423584 and Discriminator loss: 0.7592204809188843\n",
            "For Epoch: 221, Generator loss: 5.748750686645508 and Discriminator loss: 0.9069016575813293\n",
            "For Epoch: 222, Generator loss: 5.656517505645752 and Discriminator loss: 0.8243501782417297\n",
            "For Epoch: 223, Generator loss: 6.006883144378662 and Discriminator loss: 0.762538731098175\n",
            "For Epoch: 224, Generator loss: 5.728062629699707 and Discriminator loss: 0.7249785661697388\n",
            "For Epoch: 225, Generator loss: 6.1633100509643555 and Discriminator loss: 0.7996698021888733\n",
            "For Epoch: 226, Generator loss: 5.728940486907959 and Discriminator loss: 0.779870331287384\n",
            "For Epoch: 227, Generator loss: 5.1892290115356445 and Discriminator loss: 0.8255278468132019\n",
            "For Epoch: 228, Generator loss: 5.750654697418213 and Discriminator loss: 0.8135603070259094\n",
            "For Epoch: 229, Generator loss: 5.908329963684082 and Discriminator loss: 0.8779204487800598\n",
            "For Epoch: 230, Generator loss: 5.864295959472656 and Discriminator loss: 0.7252035140991211\n",
            "For Epoch: 231, Generator loss: 6.175553798675537 and Discriminator loss: 0.8333020210266113\n",
            "For Epoch: 232, Generator loss: 5.54647159576416 and Discriminator loss: 0.8546370267868042\n",
            "For Epoch: 233, Generator loss: 5.639379024505615 and Discriminator loss: 0.8158044815063477\n",
            "For Epoch: 234, Generator loss: 5.2170538902282715 and Discriminator loss: 0.8211953043937683\n",
            "For Epoch: 235, Generator loss: 5.844504356384277 and Discriminator loss: 0.7923068404197693\n",
            "For Epoch: 236, Generator loss: 5.539395809173584 and Discriminator loss: 0.8764513731002808\n",
            "For Epoch: 237, Generator loss: 5.6501922607421875 and Discriminator loss: 0.9032958745956421\n",
            "For Epoch: 238, Generator loss: 5.789969444274902 and Discriminator loss: 0.7734087705612183\n",
            "For Epoch: 239, Generator loss: 5.419713020324707 and Discriminator loss: 0.8222158551216125\n",
            "For Epoch: 240, Generator loss: 5.285059928894043 and Discriminator loss: 0.8914434313774109\n",
            "For Epoch: 241, Generator loss: 6.332643032073975 and Discriminator loss: 0.7661384344100952\n",
            "For Epoch: 242, Generator loss: 5.639300346374512 and Discriminator loss: 0.7983361482620239\n",
            "For Epoch: 243, Generator loss: 5.869466304779053 and Discriminator loss: 0.7698356509208679\n",
            "For Epoch: 244, Generator loss: 5.822436332702637 and Discriminator loss: 0.7620618939399719\n",
            "For Epoch: 245, Generator loss: 5.970353603363037 and Discriminator loss: 0.7685366868972778\n",
            "For Epoch: 246, Generator loss: 5.495475769042969 and Discriminator loss: 0.7802485227584839\n",
            "For Epoch: 247, Generator loss: 5.918763160705566 and Discriminator loss: 0.839896559715271\n",
            "For Epoch: 248, Generator loss: 6.0323944091796875 and Discriminator loss: 0.811318039894104\n",
            "For Epoch: 249, Generator loss: 5.759589672088623 and Discriminator loss: 0.8329790830612183\n",
            "For Epoch: 250, Generator loss: 5.662248134613037 and Discriminator loss: 0.8647254109382629\n",
            "For Epoch: 251, Generator loss: 6.2946343421936035 and Discriminator loss: 0.9212740063667297\n",
            "For Epoch: 252, Generator loss: 5.822175025939941 and Discriminator loss: 0.817752480506897\n",
            "For Epoch: 253, Generator loss: 6.358298301696777 and Discriminator loss: 0.8460551500320435\n",
            "For Epoch: 254, Generator loss: 5.980792999267578 and Discriminator loss: 0.7998326420783997\n",
            "For Epoch: 255, Generator loss: 5.882846355438232 and Discriminator loss: 0.7530183792114258\n",
            "For Epoch: 256, Generator loss: 5.717473030090332 and Discriminator loss: 0.8422644734382629\n",
            "For Epoch: 257, Generator loss: 5.974867820739746 and Discriminator loss: 0.7977803349494934\n",
            "For Epoch: 258, Generator loss: 6.276846408843994 and Discriminator loss: 0.7500385046005249\n",
            "For Epoch: 259, Generator loss: 6.148680686950684 and Discriminator loss: 0.7193523645401001\n",
            "For Epoch: 260, Generator loss: 5.887831687927246 and Discriminator loss: 0.7805527448654175\n",
            "For Epoch: 261, Generator loss: 6.496401309967041 and Discriminator loss: 0.7509748339653015\n",
            "For Epoch: 262, Generator loss: 6.230151176452637 and Discriminator loss: 0.838226318359375\n",
            "For Epoch: 263, Generator loss: 5.8441009521484375 and Discriminator loss: 0.7359963655471802\n",
            "For Epoch: 264, Generator loss: 6.612807273864746 and Discriminator loss: 0.7580713629722595\n",
            "For Epoch: 265, Generator loss: 6.331662654876709 and Discriminator loss: 0.7304735779762268\n",
            "For Epoch: 266, Generator loss: 5.405823707580566 and Discriminator loss: 0.7776597142219543\n",
            "For Epoch: 267, Generator loss: 6.418587684631348 and Discriminator loss: 0.7992455363273621\n",
            "For Epoch: 268, Generator loss: 5.830019950866699 and Discriminator loss: 0.7824010252952576\n",
            "For Epoch: 269, Generator loss: 5.928823471069336 and Discriminator loss: 0.8375713229179382\n",
            "For Epoch: 270, Generator loss: 5.6880083084106445 and Discriminator loss: 0.837385356426239\n",
            "For Epoch: 271, Generator loss: 6.280917167663574 and Discriminator loss: 0.6775130033493042\n",
            "For Epoch: 272, Generator loss: 5.602746963500977 and Discriminator loss: 0.8086954355239868\n",
            "For Epoch: 273, Generator loss: 5.349484920501709 and Discriminator loss: 0.8033756017684937\n",
            "For Epoch: 274, Generator loss: 5.850466251373291 and Discriminator loss: 0.8279916048049927\n",
            "For Epoch: 275, Generator loss: 5.037151336669922 and Discriminator loss: 0.8428974151611328\n",
            "For Epoch: 276, Generator loss: 5.232248306274414 and Discriminator loss: 0.833809494972229\n",
            "For Epoch: 277, Generator loss: 5.443917751312256 and Discriminator loss: 0.9242340326309204\n",
            "For Epoch: 278, Generator loss: 6.085823059082031 and Discriminator loss: 0.7830010652542114\n",
            "For Epoch: 279, Generator loss: 5.76151180267334 and Discriminator loss: 0.7467919588088989\n",
            "For Epoch: 280, Generator loss: 5.7253737449646 and Discriminator loss: 0.7616890072822571\n",
            "For Epoch: 281, Generator loss: 5.522702217102051 and Discriminator loss: 0.8789135217666626\n",
            "For Epoch: 282, Generator loss: 6.114731788635254 and Discriminator loss: 0.7678900957107544\n",
            "For Epoch: 283, Generator loss: 5.900603771209717 and Discriminator loss: 0.8292919993400574\n",
            "For Epoch: 284, Generator loss: 5.53212308883667 and Discriminator loss: 0.8478150367736816\n",
            "For Epoch: 285, Generator loss: 6.23929500579834 and Discriminator loss: 0.8162893056869507\n",
            "For Epoch: 286, Generator loss: 5.662189483642578 and Discriminator loss: 0.827593207359314\n",
            "For Epoch: 287, Generator loss: 6.366738319396973 and Discriminator loss: 0.8148086667060852\n",
            "For Epoch: 288, Generator loss: 5.7376251220703125 and Discriminator loss: 0.8553045392036438\n",
            "For Epoch: 289, Generator loss: 5.99808406829834 and Discriminator loss: 0.7848216891288757\n",
            "For Epoch: 290, Generator loss: 5.564486980438232 and Discriminator loss: 0.7929182648658752\n",
            "For Epoch: 291, Generator loss: 6.07904577255249 and Discriminator loss: 0.7657613158226013\n",
            "For Epoch: 292, Generator loss: 5.866259574890137 and Discriminator loss: 0.7674669027328491\n",
            "For Epoch: 293, Generator loss: 5.445218563079834 and Discriminator loss: 0.7770365476608276\n",
            "For Epoch: 294, Generator loss: 6.512948036193848 and Discriminator loss: 0.8924157023429871\n",
            "For Epoch: 295, Generator loss: 6.617192268371582 and Discriminator loss: 0.8768346905708313\n",
            "For Epoch: 296, Generator loss: 6.185062885284424 and Discriminator loss: 0.7695332765579224\n",
            "For Epoch: 297, Generator loss: 6.3231401443481445 and Discriminator loss: 0.7089824676513672\n",
            "For Epoch: 298, Generator loss: 6.056334495544434 and Discriminator loss: 0.8474909067153931\n",
            "For Epoch: 299, Generator loss: 6.127470970153809 and Discriminator loss: 0.7904337644577026\n",
            "For Epoch: 300, Generator loss: 6.659505367279053 and Discriminator loss: 0.7942858934402466\n",
            "For Epoch: 301, Generator loss: 5.574315071105957 and Discriminator loss: 0.8493097424507141\n",
            "For Epoch: 302, Generator loss: 6.019472599029541 and Discriminator loss: 0.7357712984085083\n",
            "For Epoch: 303, Generator loss: 6.37720251083374 and Discriminator loss: 0.7984709739685059\n",
            "For Epoch: 304, Generator loss: 5.691504001617432 and Discriminator loss: 0.8205884695053101\n",
            "For Epoch: 305, Generator loss: 6.150655269622803 and Discriminator loss: 0.8202990293502808\n",
            "For Epoch: 306, Generator loss: 6.912686347961426 and Discriminator loss: 0.8050651550292969\n",
            "For Epoch: 307, Generator loss: 6.397458076477051 and Discriminator loss: 0.769438624382019\n",
            "For Epoch: 308, Generator loss: 6.236730098724365 and Discriminator loss: 0.7680997848510742\n",
            "For Epoch: 309, Generator loss: 5.62433385848999 and Discriminator loss: 0.8128994107246399\n",
            "For Epoch: 310, Generator loss: 6.152358055114746 and Discriminator loss: 0.7524194121360779\n",
            "For Epoch: 311, Generator loss: 5.774026393890381 and Discriminator loss: 0.8204537630081177\n",
            "For Epoch: 312, Generator loss: 6.1360883712768555 and Discriminator loss: 0.79755699634552\n",
            "For Epoch: 313, Generator loss: 5.502508640289307 and Discriminator loss: 0.826574444770813\n",
            "For Epoch: 314, Generator loss: 6.233510971069336 and Discriminator loss: 0.7795608043670654\n",
            "For Epoch: 315, Generator loss: 5.953622817993164 and Discriminator loss: 0.783089816570282\n",
            "For Epoch: 316, Generator loss: 5.46926212310791 and Discriminator loss: 0.7784635424613953\n",
            "For Epoch: 317, Generator loss: 5.907376766204834 and Discriminator loss: 0.7849401235580444\n",
            "For Epoch: 318, Generator loss: 6.034111976623535 and Discriminator loss: 0.7492230534553528\n",
            "For Epoch: 319, Generator loss: 5.145051002502441 and Discriminator loss: 0.8836382031440735\n",
            "For Epoch: 320, Generator loss: 6.516212463378906 and Discriminator loss: 0.7979815006256104\n",
            "For Epoch: 321, Generator loss: 5.9439215660095215 and Discriminator loss: 0.7252102494239807\n",
            "For Epoch: 322, Generator loss: 6.002798557281494 and Discriminator loss: 0.8187150955200195\n",
            "For Epoch: 323, Generator loss: 6.379427433013916 and Discriminator loss: 0.7350329160690308\n",
            "For Epoch: 324, Generator loss: 6.473252296447754 and Discriminator loss: 0.7564075589179993\n",
            "For Epoch: 325, Generator loss: 5.662023067474365 and Discriminator loss: 0.8973987698554993\n",
            "For Epoch: 326, Generator loss: 6.233403205871582 and Discriminator loss: 0.7296959757804871\n",
            "For Epoch: 327, Generator loss: 5.766147613525391 and Discriminator loss: 0.7690829038619995\n",
            "For Epoch: 328, Generator loss: 6.217887878417969 and Discriminator loss: 0.8435195684432983\n",
            "For Epoch: 329, Generator loss: 5.371181488037109 and Discriminator loss: 0.8066201210021973\n",
            "For Epoch: 330, Generator loss: 6.4971747398376465 and Discriminator loss: 0.7776790857315063\n",
            "For Epoch: 331, Generator loss: 5.817854404449463 and Discriminator loss: 0.7694186568260193\n",
            "For Epoch: 332, Generator loss: 6.728854179382324 and Discriminator loss: 0.722671627998352\n",
            "For Epoch: 333, Generator loss: 5.5600786209106445 and Discriminator loss: 0.8285919427871704\n",
            "For Epoch: 334, Generator loss: 6.722229957580566 and Discriminator loss: 0.7247734069824219\n",
            "For Epoch: 335, Generator loss: 6.091940879821777 and Discriminator loss: 0.7994495034217834\n",
            "For Epoch: 336, Generator loss: 5.928353786468506 and Discriminator loss: 0.7416220307350159\n",
            "For Epoch: 337, Generator loss: 6.613488674163818 and Discriminator loss: 0.7658190727233887\n",
            "For Epoch: 338, Generator loss: 6.621175289154053 and Discriminator loss: 0.8503090143203735\n",
            "For Epoch: 339, Generator loss: 5.84699010848999 and Discriminator loss: 0.8559994697570801\n",
            "For Epoch: 340, Generator loss: 6.261109352111816 and Discriminator loss: 0.7047788500785828\n",
            "For Epoch: 341, Generator loss: 6.680764198303223 and Discriminator loss: 0.7295917272567749\n",
            "For Epoch: 342, Generator loss: 6.221451759338379 and Discriminator loss: 0.7345694303512573\n",
            "For Epoch: 343, Generator loss: 6.3894758224487305 and Discriminator loss: 0.7764051556587219\n",
            "For Epoch: 344, Generator loss: 6.440863609313965 and Discriminator loss: 0.7041252851486206\n",
            "For Epoch: 345, Generator loss: 6.665316104888916 and Discriminator loss: 0.774573564529419\n",
            "For Epoch: 346, Generator loss: 5.8498125076293945 and Discriminator loss: 0.6989272832870483\n",
            "For Epoch: 347, Generator loss: 5.876662254333496 and Discriminator loss: 0.7623480558395386\n",
            "For Epoch: 348, Generator loss: 6.299925804138184 and Discriminator loss: 0.7625738382339478\n",
            "For Epoch: 349, Generator loss: 6.555767059326172 and Discriminator loss: 0.7148451805114746\n",
            "For Epoch: 350, Generator loss: 5.859761714935303 and Discriminator loss: 0.7022866606712341\n",
            "For Epoch: 351, Generator loss: 5.608767986297607 and Discriminator loss: 0.7046783566474915\n",
            "For Epoch: 352, Generator loss: 6.600870609283447 and Discriminator loss: 0.7299562692642212\n",
            "For Epoch: 353, Generator loss: 6.177401065826416 and Discriminator loss: 0.8434723019599915\n",
            "For Epoch: 354, Generator loss: 6.052812099456787 and Discriminator loss: 0.8157100677490234\n",
            "For Epoch: 355, Generator loss: 6.784090995788574 and Discriminator loss: 0.676704466342926\n",
            "For Epoch: 356, Generator loss: 5.921218395233154 and Discriminator loss: 0.794326901435852\n",
            "For Epoch: 357, Generator loss: 5.795382499694824 and Discriminator loss: 0.8589725494384766\n",
            "For Epoch: 358, Generator loss: 5.750164031982422 and Discriminator loss: 0.8302048444747925\n",
            "For Epoch: 359, Generator loss: 6.9990692138671875 and Discriminator loss: 0.7842037081718445\n",
            "For Epoch: 360, Generator loss: 6.802495002746582 and Discriminator loss: 0.7947521805763245\n",
            "For Epoch: 361, Generator loss: 6.547566890716553 and Discriminator loss: 0.7636978030204773\n",
            "For Epoch: 362, Generator loss: 5.766814231872559 and Discriminator loss: 0.735286295413971\n",
            "For Epoch: 363, Generator loss: 6.031990051269531 and Discriminator loss: 0.8006223440170288\n",
            "For Epoch: 364, Generator loss: 6.28287410736084 and Discriminator loss: 0.8127726316452026\n",
            "For Epoch: 365, Generator loss: 6.088191032409668 and Discriminator loss: 0.7855483889579773\n",
            "For Epoch: 366, Generator loss: 5.1902666091918945 and Discriminator loss: 0.7513676881790161\n",
            "For Epoch: 367, Generator loss: 6.1200785636901855 and Discriminator loss: 0.8669649958610535\n",
            "For Epoch: 368, Generator loss: 5.699008941650391 and Discriminator loss: 0.7669265866279602\n",
            "For Epoch: 369, Generator loss: 5.83503532409668 and Discriminator loss: 0.7658547163009644\n",
            "For Epoch: 370, Generator loss: 6.13336181640625 and Discriminator loss: 0.8021003007888794\n",
            "For Epoch: 371, Generator loss: 6.2045183181762695 and Discriminator loss: 0.781263530254364\n",
            "For Epoch: 372, Generator loss: 6.23358154296875 and Discriminator loss: 0.7456494569778442\n",
            "For Epoch: 373, Generator loss: 6.686110496520996 and Discriminator loss: 0.6647273898124695\n",
            "For Epoch: 374, Generator loss: 6.504173278808594 and Discriminator loss: 0.7871602177619934\n",
            "For Epoch: 375, Generator loss: 6.534881591796875 and Discriminator loss: 0.8344775438308716\n",
            "For Epoch: 376, Generator loss: 5.850522994995117 and Discriminator loss: 0.8543513417243958\n",
            "For Epoch: 377, Generator loss: 6.0453691482543945 and Discriminator loss: 0.6904010772705078\n",
            "For Epoch: 378, Generator loss: 5.850388050079346 and Discriminator loss: 0.7236939668655396\n",
            "For Epoch: 379, Generator loss: 6.5191969871521 and Discriminator loss: 0.7414939403533936\n",
            "For Epoch: 380, Generator loss: 6.115802764892578 and Discriminator loss: 0.8363224267959595\n",
            "For Epoch: 381, Generator loss: 6.151866436004639 and Discriminator loss: 0.7255743741989136\n",
            "For Epoch: 382, Generator loss: 6.459733486175537 and Discriminator loss: 0.7392293810844421\n",
            "For Epoch: 383, Generator loss: 6.317998886108398 and Discriminator loss: 0.746137261390686\n",
            "For Epoch: 384, Generator loss: 6.123878479003906 and Discriminator loss: 0.7909971475601196\n",
            "For Epoch: 385, Generator loss: 5.948616981506348 and Discriminator loss: 0.7886292338371277\n",
            "For Epoch: 386, Generator loss: 5.6828107833862305 and Discriminator loss: 0.8158403635025024\n",
            "For Epoch: 387, Generator loss: 6.572259426116943 and Discriminator loss: 0.7861466407775879\n",
            "For Epoch: 388, Generator loss: 6.764522552490234 and Discriminator loss: 0.7937353849411011\n",
            "For Epoch: 389, Generator loss: 6.264803409576416 and Discriminator loss: 0.7694591879844666\n",
            "For Epoch: 390, Generator loss: 6.300227642059326 and Discriminator loss: 0.7488728761672974\n",
            "For Epoch: 391, Generator loss: 6.388748645782471 and Discriminator loss: 0.7645150423049927\n",
            "For Epoch: 392, Generator loss: 5.904346466064453 and Discriminator loss: 0.7486335635185242\n",
            "For Epoch: 393, Generator loss: 6.369254112243652 and Discriminator loss: 0.714957058429718\n",
            "For Epoch: 394, Generator loss: 6.323563575744629 and Discriminator loss: 0.7327743768692017\n",
            "For Epoch: 395, Generator loss: 6.78955602645874 and Discriminator loss: 0.8321012258529663\n",
            "For Epoch: 396, Generator loss: 6.488114833831787 and Discriminator loss: 0.7158278822898865\n",
            "For Epoch: 397, Generator loss: 6.478490352630615 and Discriminator loss: 0.762076735496521\n",
            "For Epoch: 398, Generator loss: 5.977400779724121 and Discriminator loss: 0.7767142057418823\n",
            "For Epoch: 399, Generator loss: 6.242894172668457 and Discriminator loss: 0.8040633201599121\n",
            "For Epoch: 400, Generator loss: 6.321318626403809 and Discriminator loss: 0.7395038604736328\n",
            "For Epoch: 401, Generator loss: 6.158050537109375 and Discriminator loss: 0.7022807598114014\n",
            "For Epoch: 402, Generator loss: 6.648662567138672 and Discriminator loss: 0.7332180738449097\n",
            "For Epoch: 403, Generator loss: 6.682947635650635 and Discriminator loss: 0.732753574848175\n",
            "For Epoch: 404, Generator loss: 6.0254292488098145 and Discriminator loss: 0.7346736788749695\n",
            "For Epoch: 405, Generator loss: 6.243945121765137 and Discriminator loss: 0.7646495699882507\n",
            "For Epoch: 406, Generator loss: 6.507723331451416 and Discriminator loss: 0.7548385858535767\n",
            "For Epoch: 407, Generator loss: 5.612085819244385 and Discriminator loss: 0.8081766963005066\n",
            "For Epoch: 408, Generator loss: 6.626082420349121 and Discriminator loss: 0.7038942575454712\n",
            "For Epoch: 409, Generator loss: 6.298611164093018 and Discriminator loss: 0.7574995756149292\n",
            "For Epoch: 410, Generator loss: 6.556769371032715 and Discriminator loss: 0.8669995069503784\n",
            "For Epoch: 411, Generator loss: 6.872951507568359 and Discriminator loss: 0.7405543327331543\n",
            "For Epoch: 412, Generator loss: 5.610950469970703 and Discriminator loss: 0.7326273322105408\n",
            "For Epoch: 413, Generator loss: 5.951268196105957 and Discriminator loss: 0.7602527737617493\n",
            "For Epoch: 414, Generator loss: 5.845861434936523 and Discriminator loss: 0.7966273427009583\n",
            "For Epoch: 415, Generator loss: 6.7240891456604 and Discriminator loss: 0.7783385515213013\n",
            "For Epoch: 416, Generator loss: 6.354542255401611 and Discriminator loss: 0.8432514071464539\n",
            "For Epoch: 417, Generator loss: 6.403958320617676 and Discriminator loss: 0.7272419333457947\n",
            "For Epoch: 418, Generator loss: 6.7681779861450195 and Discriminator loss: 0.7474501132965088\n",
            "For Epoch: 419, Generator loss: 6.378777980804443 and Discriminator loss: 0.7227352261543274\n",
            "For Epoch: 420, Generator loss: 5.914944171905518 and Discriminator loss: 0.7403587102890015\n",
            "For Epoch: 421, Generator loss: 6.839259147644043 and Discriminator loss: 0.7766727805137634\n",
            "For Epoch: 422, Generator loss: 5.970485687255859 and Discriminator loss: 0.8088840246200562\n",
            "For Epoch: 423, Generator loss: 5.867583274841309 and Discriminator loss: 0.7771488428115845\n",
            "For Epoch: 424, Generator loss: 6.882390022277832 and Discriminator loss: 0.6827391386032104\n",
            "For Epoch: 425, Generator loss: 6.447634220123291 and Discriminator loss: 0.7317109107971191\n",
            "For Epoch: 426, Generator loss: 6.634340763092041 and Discriminator loss: 0.7090092897415161\n",
            "For Epoch: 427, Generator loss: 5.814755439758301 and Discriminator loss: 0.7908172607421875\n",
            "For Epoch: 428, Generator loss: 6.621804714202881 and Discriminator loss: 0.8205701112747192\n",
            "For Epoch: 429, Generator loss: 6.790732383728027 and Discriminator loss: 0.7530211210250854\n",
            "For Epoch: 430, Generator loss: 6.5423102378845215 and Discriminator loss: 0.7777036428451538\n",
            "For Epoch: 431, Generator loss: 6.276597499847412 and Discriminator loss: 0.6625919938087463\n",
            "For Epoch: 432, Generator loss: 6.423824310302734 and Discriminator loss: 0.8251398205757141\n",
            "For Epoch: 433, Generator loss: 6.462302207946777 and Discriminator loss: 0.7746797800064087\n",
            "For Epoch: 434, Generator loss: 6.110633850097656 and Discriminator loss: 0.7107187509536743\n",
            "For Epoch: 435, Generator loss: 6.146115303039551 and Discriminator loss: 0.6749869585037231\n",
            "For Epoch: 436, Generator loss: 6.314947605133057 and Discriminator loss: 0.6946911215782166\n",
            "For Epoch: 437, Generator loss: 7.026876926422119 and Discriminator loss: 0.7727464437484741\n",
            "For Epoch: 438, Generator loss: 5.356162071228027 and Discriminator loss: 0.8418374061584473\n",
            "For Epoch: 439, Generator loss: 6.266053199768066 and Discriminator loss: 0.7331902384757996\n",
            "For Epoch: 440, Generator loss: 6.0273518562316895 and Discriminator loss: 0.8392633199691772\n",
            "For Epoch: 441, Generator loss: 6.700067043304443 and Discriminator loss: 0.7839139699935913\n",
            "For Epoch: 442, Generator loss: 6.945069789886475 and Discriminator loss: 0.7482959032058716\n",
            "For Epoch: 443, Generator loss: 6.476618766784668 and Discriminator loss: 0.7721907496452332\n",
            "For Epoch: 444, Generator loss: 6.7636308670043945 and Discriminator loss: 0.7736203670501709\n",
            "For Epoch: 445, Generator loss: 5.6491241455078125 and Discriminator loss: 0.7858127355575562\n",
            "For Epoch: 446, Generator loss: 6.769975185394287 and Discriminator loss: 0.7159293293952942\n",
            "For Epoch: 447, Generator loss: 5.727304458618164 and Discriminator loss: 0.8008772134780884\n",
            "For Epoch: 448, Generator loss: 5.972453594207764 and Discriminator loss: 0.7053272128105164\n",
            "For Epoch: 449, Generator loss: 6.329251766204834 and Discriminator loss: 0.7443646192550659\n",
            "For Epoch: 450, Generator loss: 6.263978004455566 and Discriminator loss: 0.7798577547073364\n",
            "For Epoch: 451, Generator loss: 5.946045875549316 and Discriminator loss: 0.8059752583503723\n",
            "For Epoch: 452, Generator loss: 6.301355361938477 and Discriminator loss: 0.8212372660636902\n",
            "For Epoch: 453, Generator loss: 6.922116279602051 and Discriminator loss: 0.774651825428009\n",
            "For Epoch: 454, Generator loss: 5.879210472106934 and Discriminator loss: 0.8001201748847961\n",
            "For Epoch: 455, Generator loss: 6.20035457611084 and Discriminator loss: 0.6970131993293762\n",
            "For Epoch: 456, Generator loss: 6.017539978027344 and Discriminator loss: 0.7000622749328613\n",
            "For Epoch: 457, Generator loss: 6.258802890777588 and Discriminator loss: 0.8227834701538086\n",
            "For Epoch: 458, Generator loss: 6.233029842376709 and Discriminator loss: 0.7784780263900757\n",
            "For Epoch: 459, Generator loss: 7.492331027984619 and Discriminator loss: 0.7027539014816284\n",
            "For Epoch: 460, Generator loss: 6.70041561126709 and Discriminator loss: 0.7393385767936707\n",
            "For Epoch: 461, Generator loss: 6.216274738311768 and Discriminator loss: 0.699390172958374\n",
            "For Epoch: 462, Generator loss: 6.480307102203369 and Discriminator loss: 0.7808868288993835\n",
            "For Epoch: 463, Generator loss: 6.395776271820068 and Discriminator loss: 0.7029833793640137\n",
            "For Epoch: 464, Generator loss: 5.741174697875977 and Discriminator loss: 0.7394919395446777\n",
            "For Epoch: 465, Generator loss: 6.028338432312012 and Discriminator loss: 0.6995208859443665\n",
            "For Epoch: 466, Generator loss: 6.631608009338379 and Discriminator loss: 0.8009912371635437\n",
            "For Epoch: 467, Generator loss: 6.493773460388184 and Discriminator loss: 0.7423833608627319\n",
            "For Epoch: 468, Generator loss: 5.926182746887207 and Discriminator loss: 0.808239758014679\n",
            "For Epoch: 469, Generator loss: 5.569095611572266 and Discriminator loss: 0.7824897766113281\n",
            "For Epoch: 470, Generator loss: 6.862338066101074 and Discriminator loss: 0.6212121248245239\n",
            "For Epoch: 471, Generator loss: 6.271153450012207 and Discriminator loss: 0.8063686490058899\n",
            "For Epoch: 472, Generator loss: 6.54840087890625 and Discriminator loss: 0.7056963443756104\n",
            "For Epoch: 473, Generator loss: 6.454255104064941 and Discriminator loss: 0.6946128010749817\n",
            "For Epoch: 474, Generator loss: 5.705069541931152 and Discriminator loss: 0.7207915782928467\n",
            "For Epoch: 475, Generator loss: 6.41284704208374 and Discriminator loss: 0.7126896977424622\n",
            "For Epoch: 476, Generator loss: 7.021481990814209 and Discriminator loss: 0.7668024301528931\n",
            "For Epoch: 477, Generator loss: 6.017829418182373 and Discriminator loss: 0.7460669279098511\n",
            "For Epoch: 478, Generator loss: 5.61845588684082 and Discriminator loss: 0.7729407548904419\n",
            "For Epoch: 479, Generator loss: 6.831180572509766 and Discriminator loss: 0.7890115976333618\n",
            "For Epoch: 480, Generator loss: 7.021182060241699 and Discriminator loss: 0.7134870290756226\n",
            "For Epoch: 481, Generator loss: 7.070412635803223 and Discriminator loss: 0.7834765315055847\n",
            "For Epoch: 482, Generator loss: 6.431174278259277 and Discriminator loss: 0.7891753315925598\n",
            "For Epoch: 483, Generator loss: 6.931178092956543 and Discriminator loss: 0.7303192615509033\n",
            "For Epoch: 484, Generator loss: 6.5347795486450195 and Discriminator loss: 0.9224096536636353\n",
            "For Epoch: 485, Generator loss: 6.63309383392334 and Discriminator loss: 0.7022123336791992\n",
            "For Epoch: 486, Generator loss: 6.381386756896973 and Discriminator loss: 0.830163836479187\n",
            "For Epoch: 487, Generator loss: 6.291696548461914 and Discriminator loss: 0.8166667222976685\n",
            "For Epoch: 488, Generator loss: 6.694802761077881 and Discriminator loss: 0.8116059303283691\n",
            "For Epoch: 489, Generator loss: 6.275749206542969 and Discriminator loss: 0.7473889589309692\n",
            "For Epoch: 490, Generator loss: 5.924270153045654 and Discriminator loss: 0.8400020599365234\n",
            "For Epoch: 491, Generator loss: 6.4822211265563965 and Discriminator loss: 0.7051327228546143\n",
            "For Epoch: 492, Generator loss: 5.660447120666504 and Discriminator loss: 0.7914208769798279\n",
            "For Epoch: 493, Generator loss: 6.853801727294922 and Discriminator loss: 0.6616613268852234\n",
            "For Epoch: 494, Generator loss: 6.277170181274414 and Discriminator loss: 0.7739260792732239\n",
            "For Epoch: 495, Generator loss: 6.320260047912598 and Discriminator loss: 0.7461814880371094\n",
            "For Epoch: 496, Generator loss: 6.6879682540893555 and Discriminator loss: 0.6330078840255737\n",
            "For Epoch: 497, Generator loss: 6.738150119781494 and Discriminator loss: 0.6942530274391174\n",
            "For Epoch: 498, Generator loss: 6.986880302429199 and Discriminator loss: 0.816103458404541\n",
            "For Epoch: 499, Generator loss: 6.293734550476074 and Discriminator loss: 0.7333434224128723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, train_pred = discriminator(tf.convert_to_tensor(x_train))\n",
        "_, test_pred = discriminator(tf.convert_to_tensor(x_test))\n",
        "binary_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "binary_accuracy.update_state(tf.one_hot(tf.squeeze(y_train), depth=2), train_pred)\n",
        "print('Train accuracy: %.3f%s'% (binary_accuracy.result().numpy()*100, '%'))\n",
        "binary_accuracy.update_state(tf.one_hot(tf.squeeze(y_test), depth=2), test_pred)\n",
        "print('Test accuracy: %.3f%s'% (binary_accuracy.result().numpy()*100, '%'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPWMvntryB0M",
        "outputId": "d98f2ea7-b44b-4188-8a5f-70813d474b4c"
      },
      "id": "GPWMvntryB0M",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 82.000%\n",
            "Test accuracy: 76.500%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Discriminator vs Generator loss')\n",
        "plt.plot(dis_losse, label='Discriminator loss')\n",
        "plt.plot(gen_losse, label='Generator loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Hg56QBaViGjk",
        "outputId": "5d03123b-35b4-4154-8194-386d093c46ba"
      },
      "id": "Hg56QBaViGjk",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xzC6dSAziGpO"
      },
      "id": "xzC6dSAziGpO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Task_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}